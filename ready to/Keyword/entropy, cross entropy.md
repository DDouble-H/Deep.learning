정보이론에서 엔트로피(Entropy)는 불확실성의 척도를 말한다. 정보이론에서 정보량은 자주 발생하는 관찰이나 사건에 대해 작은 값을 주고, 자주 발생하지 않는 사건에 대해서는 큰 값을 준다. 즉, 엔트로피가 높다는 것은 정보가 많고 확률이 낮다는 것을 의미한다. 머신러닝에서 엔트로피는 주어진 정보에 얼마나 비슷한 요소들이 있는지를 측정하는 척도로 사용될 수 있다. 크로스엔트로피(Cross-entropy)는 실제값과 예측값의 차이를 줄이기 위한 엔트로피로 크로스 엔트로피를 실제값과 예측값간의 차이를 최소화 시키는 방향으로 학습한다.

